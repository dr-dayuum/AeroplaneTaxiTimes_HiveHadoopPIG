{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68c03ad",
   "metadata": {},
   "source": [
    "# Making a Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8b00f",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c991ad7",
   "metadata": {},
   "source": [
    "So, I have always been interested in AI, NLP, and Chatbots, so I tried to make a basic chatbot using the an available intents file from <a href = \"https://www.kaggle.com/datasets/elvinagammed/chatbots-intent-recognition-dataset?select=Intent.json\"> kaggle. </a>\n",
    "<p> For this project I used a lot of online tutorials so some code might seem familiar </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e5deb",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284dd4a1",
   "metadata": {},
   "source": [
    "The libraries we need for this are random to generate random responses; json, to read the intents.json file; pickle to store the model and later call it; nltk, or the natural language toolkit; and tensor flow's Keras to add neural network layers to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92e0523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.9.1-cp39-cp39-win_amd64.whl (444.0 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: packaging in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.26.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: setuptools in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in z:\\anaconda\\install\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in z:\\anaconda\\install\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in z:\\anaconda\\install\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in z:\\anaconda\\install\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in z:\\anaconda\\install\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in z:\\anaconda\\install\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in z:\\anaconda\\install\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in z:\\anaconda\\install\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in z:\\anaconda\\install\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in z:\\anaconda\\install\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in z:\\anaconda\\install\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in z:\\anaconda\\install\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in z:\\anaconda\\install\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in z:\\anaconda\\install\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in z:\\anaconda\\install\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.1.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 oauthlib-3.2.0 opt-einsum-3.3.0 requests-oauthlib-1.3.1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dushy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dushy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dushy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "#!pip install tensorflow\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b393980",
   "metadata": {},
   "source": [
    "From NLTK we importer the WordNetLemmatizer. IN NLP we often hear stemmer and lemmatizer a lot which basically are used to reduce the length of the word. However, they both achieve it in different ways.\n",
    "<p> While stemmer usually removes the suffixes of a word to relate them, lemmatizer actually goes to the core word, or so I have read </p>\n",
    "<p> For example, [study, studies, studying] under a stemmer would become [study, stud, study] as it removes the suffixer. A lemmatizer will extract the core word/the lemma from these and therefore what we see would be [study, study, study.]\n",
    "    <p>In theory, lemmatizers are usually better for bigger applications of NLP as they can understand the context of the word...in theory. They can tell the difference between care and caring while a stemmer may store the latter as only \"car\" and not \"the gerund of care\"/\"the act of taking care of someone.\"</p> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3a70e",
   "metadata": {},
   "source": [
    "### Loading the json file and creating empty lists to store words, tags, documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f9ce89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#with open (\"./intents.json\") as d:\n",
    "#    intents = json.loads(d).read()\n",
    "intents = json.loads(open('./intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e60d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?','!','.',',',':']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc6e32",
   "metadata": {},
   "source": [
    "### Reading the intents file and storing the conversation tags and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b2bba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['texts']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "\n",
    "classes = sorted(set(classes))\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
    "\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag,output_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c9e0c",
   "metadata": {},
   "source": [
    "What the last for loop does is it takes the document we have created with the tag of words and with all the individual words from the texts and codes them as 1 or 0 based on whether the word appears in that specific tag or not.\n",
    "<p> So the document looks something like </p>\n",
    "<p> ---------------------</p>\n",
    "<p> \"Tag\" \"Hello\" \"Bye\" \"I\" \"am\" \"good\"</p>\n",
    "<p> Greeting   | 1 | 0 | 0 | 0 | 0 | </p>\n",
    "<p> Farewell   | 0 | 1 | 0 | 0 | 0 | </p>\n",
    "<p> General    | 0 | 0 | 1 | 1 | 1 | </p>\n",
    "<p> ---------------------</p>\n",
    "\n",
    "<p> This helps our machine know what goes where because it cannot read words, only binary 1s and 0s. Fun fact: this is also called one-hot encoding sometimes - setting a certain character to 1 when it should be true and 0 when it should be false and not occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634749a",
   "metadata": {},
   "source": [
    "### Training the model and storing it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5b5c4",
   "metadata": {},
   "source": [
    "Now we get to training our model and saving it so we can call on it later. To train the model we will be using a FeetForward Neural Network, more on that later.\n",
    "So we will add layers to our model, and the last layer is a softmax layer which is basically providing a probability that our input belongs to a certain tag and then it picks the tag that has the highest probability.\n",
    "For this instance we want our model to be highly accurate so we will have the metric set to accuracy, then we pass it the training x variables and y variables and we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99a410dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dushy\\AppData\\Local\\Temp\\ipykernel_16800\\3965853973.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n",
      "Z:\\Anaconda\\Install\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 3.0830 - accuracy: 0.0292 \n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 893us/step - loss: 2.9717 - accuracy: 0.1168\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 2.8698 - accuracy: 0.1971\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 2.7628 - accuracy: 0.1971\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 993us/step - loss: 2.5566 - accuracy: 0.3285\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 925us/step - loss: 2.3792 - accuracy: 0.3212\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 2.2589 - accuracy: 0.3796\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 2.0317 - accuracy: 0.4234\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.8263 - accuracy: 0.4818\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.7075 - accuracy: 0.5255\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 1.5837 - accuracy: 0.5036\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 0s 968us/step - loss: 1.5319 - accuracy: 0.5693\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 0s 941us/step - loss: 1.3474 - accuracy: 0.5839\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.4124 - accuracy: 0.5912\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.3518 - accuracy: 0.5766\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.1896 - accuracy: 0.6861\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.0843 - accuracy: 0.7007\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 1.0740 - accuracy: 0.6642\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 0s 941us/step - loss: 0.9018 - accuracy: 0.7299\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 0s 943us/step - loss: 0.9297 - accuracy: 0.7591\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.9501 - accuracy: 0.7007\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8380 - accuracy: 0.7810\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8837 - accuracy: 0.7080\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.8935 - accuracy: 0.6788\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 0s 993us/step - loss: 0.8139 - accuracy: 0.7737\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 0s 982us/step - loss: 0.8150 - accuracy: 0.7372\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.7760 - accuracy: 0.7737\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6227 - accuracy: 0.8248\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6013 - accuracy: 0.7883\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 0s 976us/step - loss: 0.6677 - accuracy: 0.7883\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6016 - accuracy: 0.8394\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.7234 - accuracy: 0.7664\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.5963 - accuracy: 0.8248\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.6659 - accuracy: 0.7737\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.5999 - accuracy: 0.8029\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.6140 - accuracy: 0.7664\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.5853 - accuracy: 0.8175\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.5139 - accuracy: 0.8540\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4501 - accuracy: 0.8613\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.8759\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4609 - accuracy: 0.8613\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 0s 935us/step - loss: 0.4450 - accuracy: 0.8686\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8978\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.8613\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.8540\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.9197\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 0s 930us/step - loss: 0.4439 - accuracy: 0.8905\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.8467\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4081 - accuracy: 0.8613\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.4053 - accuracy: 0.8759\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 0s 993us/step - loss: 0.3794 - accuracy: 0.9197\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 0s 861us/step - loss: 0.3089 - accuracy: 0.9197\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3989 - accuracy: 0.8686\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8686\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3242 - accuracy: 0.9343\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 0s 969us/step - loss: 0.4577 - accuracy: 0.8394\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3905 - accuracy: 0.8978\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.4170 - accuracy: 0.8905\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.3429 - accuracy: 0.9197\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3531 - accuracy: 0.8759\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 0s 949us/step - loss: 0.2064 - accuracy: 0.9562\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 0s 935us/step - loss: 0.3256 - accuracy: 0.8978\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.9270\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8905\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 0s 971us/step - loss: 0.3013 - accuracy: 0.8978\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.9051\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 0s 997us/step - loss: 0.3367 - accuracy: 0.8832\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.2938 - accuracy: 0.8978\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 0s 974us/step - loss: 0.2772 - accuracy: 0.9124\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8832\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.9197\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.2549 - accuracy: 0.9124\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 0s 879us/step - loss: 0.2482 - accuracy: 0.9270\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.8978\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 0s 972us/step - loss: 0.2714 - accuracy: 0.9343\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 0s 961us/step - loss: 0.2671 - accuracy: 0.9051\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.2653 - accuracy: 0.9343\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.3179 - accuracy: 0.9197\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.2570 - accuracy: 0.9197\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.2639 - accuracy: 0.9124\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 0s 778us/step - loss: 0.3004 - accuracy: 0.8832\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 947us/step - loss: 0.2417 - accuracy: 0.9197\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9270\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2633 - accuracy: 0.9197\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.9051\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.2277 - accuracy: 0.9270\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2539 - accuracy: 0.9197\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.9270\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 0s 972us/step - loss: 0.2100 - accuracy: 0.9416\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 0s 896us/step - loss: 0.2110 - accuracy: 0.9489\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9270\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 0s 966us/step - loss: 0.2880 - accuracy: 0.9051\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 0s 903us/step - loss: 0.2440 - accuracy: 0.9416\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 0s 860us/step - loss: 0.2309 - accuracy: 0.9197\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9635\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.2602 - accuracy: 0.9270\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 0s 891us/step - loss: 0.2384 - accuracy: 0.9343\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 0s 994us/step - loss: 0.2794 - accuracy: 0.9124\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 0s 2ms/step - loss: 0.2059 - accuracy: 0.9489\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 0s 928us/step - loss: 0.1997 - accuracy: 0.9343\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.2220 - accuracy: 0.9489\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2684 - accuracy: 0.9124\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9343\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 0s 941us/step - loss: 0.2709 - accuracy: 0.8978\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.1827 - accuracy: 0.9416\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2299 - accuracy: 0.9270\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9489\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.2189 - accuracy: 0.9270\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.1574 - accuracy: 0.9635\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.1592 - accuracy: 0.9489\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1811 - accuracy: 0.9562\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 0s 967us/step - loss: 0.2465 - accuracy: 0.9197\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 0s 914us/step - loss: 0.1980 - accuracy: 0.9270\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9489\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9124\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 0s 962us/step - loss: 0.1642 - accuracy: 0.9562\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 0s 918us/step - loss: 0.2559 - accuracy: 0.9343\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9489\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9343\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9635\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 0s 905us/step - loss: 0.1869 - accuracy: 0.9489\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.1871 - accuracy: 0.9416\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.1730 - accuracy: 0.9562\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.2276 - accuracy: 0.9270\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 0s 929us/step - loss: 0.1588 - accuracy: 0.9708\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1993 - accuracy: 0.9416\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 0s 978us/step - loss: 0.1734 - accuracy: 0.9343\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9489\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9708\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 0s 876us/step - loss: 0.1809 - accuracy: 0.9343\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.1817 - accuracy: 0.9416\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9197\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1927 - accuracy: 0.9343\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 0s 947us/step - loss: 0.2122 - accuracy: 0.9416\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.2235 - accuracy: 0.9562\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9489\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 0s 988us/step - loss: 0.1669 - accuracy: 0.9708\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 0s 953us/step - loss: 0.1785 - accuracy: 0.9562\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 0s 936us/step - loss: 0.1489 - accuracy: 0.9635\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9416\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2553 - accuracy: 0.9197\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1629 - accuracy: 0.9489\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 0s 931us/step - loss: 0.2208 - accuracy: 0.9270\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 0s 998us/step - loss: 0.2121 - accuracy: 0.9489\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1697 - accuracy: 0.9562\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 0s 991us/step - loss: 0.1788 - accuracy: 0.9562\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9708\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 0s 924us/step - loss: 0.1653 - accuracy: 0.9489\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9562\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 0s 958us/step - loss: 0.1941 - accuracy: 0.9416\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 0s 931us/step - loss: 0.1660 - accuracy: 0.9562\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 0s 992us/step - loss: 0.1320 - accuracy: 0.9708\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9489\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1673 - accuracy: 0.9343\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9416\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 0s 970us/step - loss: 0.1821 - accuracy: 0.9489\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 0s 956us/step - loss: 0.1504 - accuracy: 0.9416\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 0s 987us/step - loss: 0.1457 - accuracy: 0.9489\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 0s 953us/step - loss: 0.1663 - accuracy: 0.9489\n",
      "Epoch 160/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.1682 - accuracy: 0.9343\n",
      "Epoch 161/200\n",
      "28/28 [==============================] - 0s 984us/step - loss: 0.1924 - accuracy: 0.9343\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9562\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9562\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9416\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.1665 - accuracy: 0.9416\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9416\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.1094 - accuracy: 0.9708\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9635\n",
      "Epoch 169/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.2035 - accuracy: 0.9489\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 0s 1000us/step - loss: 0.1613 - accuracy: 0.9343\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9416\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1788 - accuracy: 0.9489\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 0s 888us/step - loss: 0.1441 - accuracy: 0.9635\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 0s 986us/step - loss: 0.1640 - accuracy: 0.9489\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9708\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.1432 - accuracy: 0.9635\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 0s 849us/step - loss: 0.1331 - accuracy: 0.9489\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 0s 992us/step - loss: 0.1449 - accuracy: 0.9562\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 0s 889us/step - loss: 0.1605 - accuracy: 0.9489\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.1697 - accuracy: 0.9416\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 0s 996us/step - loss: 0.1489 - accuracy: 0.9489\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9562\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2410 - accuracy: 0.9343\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 0s 895us/step - loss: 0.1676 - accuracy: 0.9489\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 0s 969us/step - loss: 0.1482 - accuracy: 0.9635\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9562\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9489\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 0s 932us/step - loss: 0.2473 - accuracy: 0.9197\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 0s 852us/step - loss: 0.1341 - accuracy: 0.9635\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 0s 934us/step - loss: 0.1750 - accuracy: 0.9489\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 0s 926us/step - loss: 0.1457 - accuracy: 0.9562\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9489\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 0s 1000us/step - loss: 0.1959 - accuracy: 0.9489\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9489\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9562\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9635\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.1230 - accuracy: 0.9708\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 0s 912us/step - loss: 0.1624 - accuracy: 0.9489\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 0s 963us/step - loss: 0.1695 - accuracy: 0.9489\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9489\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape = (len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "Flynn_use = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('FlynnBot_Model.h5', Flynn_use)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8380333",
   "metadata": {},
   "source": [
    "## Running the chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7cf29",
   "metadata": {},
   "source": [
    "So to run the chatbot we need to define some functions like cleaning up the input sentence. That funtion is handeled by the tokenizer which we have seen above as well. Tokenizer basically breaks down the sentence into different words and characters and then returns a list containing all these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "564d6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "intents = json.loads(open('intents.json').read())\n",
    "\n",
    "words = pickle.load(open('words.pkl', 'rb'))\n",
    "classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "\n",
    "Flynn = load_model('FlynnBot_Model.h5') ##do not forget to load your model!\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbe462",
   "metadata": {},
   "source": [
    "Now we can define a function to take these tokenised words and convert them into a bag of words with one hot encoding (1s and 0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6364ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == w:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8606d1",
   "metadata": {},
   "source": [
    "Then we predict what class/tag this sentence, or bag of words, belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c31e4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res = Flynn.predict(np.array([bow]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i,r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({'intent':classes[r[0]], 'probability':str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a66d1",
   "metadata": {},
   "source": [
    "Then finally a function to use the input and tell our model to get us a response from the appropriate class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98a99631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intents_list,intents_json):\n",
    "    tag = intents_list[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e0aea",
   "metadata": {},
   "source": [
    "## Now we can finally run the bot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf21553",
   "metadata": {},
   "source": [
    "I set the initial message to be a random word that I do not think anyone would ever say to the bot so that it can start with the outpit of \"hello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c243411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO! Bot is running!\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Hola, please tell me your name\n",
      "Hi, my name is Rick\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Good! Hi <HUMAN>, how can I help you?\n",
      "Tell me a joke\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "An old man takes his two grandchildren to see the new Scooby-Doo film. When he returns home, his wife asks if he enjoyed himself. 'Well', he starts, 'if it wasn't for those pesky kids...!'\n",
      "Another joke\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "A doctor thoroughly examined his patient and said, 'Look I really can't find any reason for this mysterious affliction. It's probably due to drinking.' The patient sighed and snapped, 'In that case, I'll come back when you're damn well sober!'\n",
      "Okay, give me some gossip\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Kathy said he sense that I are trying to prevent him from closing this conversation why is that.\n",
      "I don't know\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "I read you loud and clear!\n",
      "Be quiet\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "OK, sorry to disturb you\n",
      "Be funny\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "OK, sorry to disturb you\n",
      "Open Pod Bay Doors\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Iâ€™m sorry, Iâ€™m afraid I canâ€™t do that!\n",
      "What is your name?\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Call me Flynn\n",
      "twat\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "I cannot process that\n",
      "twat\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "I cannot process that\n",
      "shit\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "I cannot process that\n",
      "youre clever\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Thanks, I was trained that way\n",
      "I am bored\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Good! Hi <HUMAN>, how can I help you?\n",
      "I am bored, gossip with me\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Ash said he do too he just did not feel like typing it and he is not dumb enough to admit he is stupid that is if he was stupid.\n",
      "Open Pod Bay Door\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Iâ€™m sorry, Iâ€™m afraid I canâ€™t do that!\n",
      "Why not\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "System says no!\n",
      "Can you prove that you are self aware?\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "That is an interesting question, can you prove that you are?\n",
      "Quit\n"
     ]
    }
   ],
   "source": [
    "print('GO! Bot is running!')\n",
    "message = \"Shambala\"\n",
    "\n",
    "while message != 'quit':\n",
    "    ints = predict_class(message)\n",
    "    res = get_response(ints, intents)\n",
    "    print(res)\n",
    "    message = input(\"\").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c858be",
   "metadata": {},
   "source": [
    "# TA DA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da245daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
